{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import pyarrow.parquet as pq\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm.notebook import tqdm\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "import cv2, os, gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Training Data\n",
    "train_paths = [\n",
    "    './data/train_image_data_0.parquet',\n",
    "    './data/train_image_data_1.parquet',\n",
    "    './data/train_image_data_2.parquet',\n",
    "    './data/train_image_data_3.parquet'\n",
    "]\n",
    "\n",
    "test_paths = [\n",
    "    './data/test_image_data_0.parquet',\n",
    "    './data/test_image_data_1.parquet',\n",
    "    './data/test_image_data_2.parquet',\n",
    "    './data/test_image_data_3.parquet'\n",
    "]\n",
    "\n",
    "HEIGHT = 137\n",
    "WIDTH = 236\n",
    "SIZE = 299"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make image dumping directory\n",
    "train_dir_path = './data/images/train'\n",
    "test_dir_path = './data/images/test'\n",
    "\n",
    "if not os.path.isdir(train_dir_path):\n",
    "    os.makedirs(train_dir_path)\n",
    "    \n",
    "if not os.path.isdir(test_dir_path):\n",
    "    os.makedirs(test_dir_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define functions to crop and resize the images\n",
    "\n",
    "def bbox(img):\n",
    "    rows = np.any(img, axis=1)\n",
    "    cols = np.any(img, axis=0)\n",
    "    rmin, rmax = np.where(rows)[0][[0, -1]]\n",
    "    cmin, cmax = np.where(cols)[0][[0, -1]]\n",
    "    return rmin, rmax, cmin, cmax\n",
    "\n",
    "def crop_resize(img, size, pad=16):\n",
    "    #crop a box around pixels large than the threshold \n",
    "    #some images contain line at the sides\n",
    "    ymin,ymax,xmin,xmax = bbox(img[5:-5,5:-5] > 80)\n",
    "    #cropping may cut too much, so we need to add it back\n",
    "    xmin = xmin - 13 if (xmin > 13) else 0\n",
    "    ymin = ymin - 10 if (ymin > 10) else 0\n",
    "    xmax = xmax + 13 if (xmax < WIDTH - 13) else WIDTH\n",
    "    ymax = ymax + 10 if (ymax < HEIGHT - 10) else HEIGHT\n",
    "    img = img[ymin:ymax,xmin:xmax]\n",
    "    #remove lo intensity pixels as noise\n",
    "    img[img < 28] = 0\n",
    "    lx, ly = xmax-xmin,ymax-ymin\n",
    "    l = max(lx,ly) + pad\n",
    "    #make sure that the aspect ratio is kept in rescaling\n",
    "    img = np.pad(img, [((l-ly)//2,), ((l-lx)//2,)], mode='constant')\n",
    "    return cv2.resize(img,(size,size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c08c4a666dbd45e09e3761c59000bf1c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=50210.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c4f6ea97e68c416cacb479a6e6fbfd47",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=50210.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8114b355b31347feac46ea6afa790b09",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=50210.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6ec4fda8b975475e8051a01a4d1cac60",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=50210.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Preprocess the Train images and write them down\n",
    "for path in train_paths:\n",
    "    df = pd.read_parquet(path)\n",
    "\n",
    "    for row in tqdm(df.values):\n",
    "        im_name = row[0]\n",
    "        image = np.array(row[1:])\n",
    "        image = 255 - image.reshape(HEIGHT, WIDTH).astype(np.uint8)\n",
    "        image = (image*(255.0/image.max())).astype(np.uint8)\n",
    "        image = crop_resize(image, SIZE, 20)\n",
    "        _, image = cv2.threshold(image,0,255,cv2.THRESH_BINARY+cv2.THRESH_OTSU)\n",
    "        image = cv2.morphologyEx(image, cv2.MORPH_OPEN, np.ones([3,3], np.uint8))\n",
    "        cv2.imwrite(f\"{os.path.join(train_dir_path, im_name)}.jpg\", image)\n",
    "    \n",
    "    del df, row, im_name, image, _\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "64858a97c7584536ada0ec1a5f308ee8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=3.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "245c8d5e426845d6a598e5a535de2463",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=3.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "60d475ea08e8489c92b435459e0971c3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=3.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b4e490d38f2f4fa0ab9118d3c122c2ae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=3.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Preprocess the Test images and write them down\n",
    "for path in test_paths:\n",
    "    df = pd.read_parquet(path)\n",
    "\n",
    "    for row in tqdm(df.values):\n",
    "        im_name = row[0]\n",
    "        image = np.array(row[1:])\n",
    "        image = 255 - image.reshape(HEIGHT, WIDTH).astype(np.uint8)\n",
    "        image = (image*(255.0/image.max())).astype(np.uint8)\n",
    "        image = crop_resize(image, SIZE, 20)\n",
    "        _, image = cv2.threshold(image,0,255,cv2.THRESH_BINARY+cv2.THRESH_OTSU)\n",
    "        image = cv2.morphologyEx(image, cv2.MORPH_OPEN, np.ones([3,3], np.uint8))\n",
    "        cv2.imwrite(f\"{os.path.join(test_dir_path, im_name)}.jpg\", image)\n",
    "    \n",
    "    del df, row, im_name, image\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read train labels\n",
    "train_label_path = \"./data/train.csv\"\n",
    "df = pd.read_csv(train_label_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image_id</th>\n",
       "      <th>grapheme_root</th>\n",
       "      <th>vowel_diacritic</th>\n",
       "      <th>consonant_diacritic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Train_0</td>\n",
       "      <td>15</td>\n",
       "      <td>9</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Train_1</td>\n",
       "      <td>159</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Train_2</td>\n",
       "      <td>22</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Train_3</td>\n",
       "      <td>53</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Train_4</td>\n",
       "      <td>71</td>\n",
       "      <td>9</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  image_id  grapheme_root  vowel_diacritic  consonant_diacritic\n",
       "0  Train_0             15                9                    5\n",
       "1  Train_1            159                0                    0\n",
       "2  Train_2             22                3                    5\n",
       "3  Train_3             53                2                    2\n",
       "4  Train_4             71                9                    5"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Dropping the last column (not required for training)\n",
    "df.drop(['grapheme'], axis=1, inplace=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separating the data from the labels and one hot encoding the labels\n",
    "y_root = pd.get_dummies(df['grapheme_root'])\n",
    "y_vowel = pd.get_dummies(df['vowel_diacritic'])\n",
    "y_consonant = pd.get_dummies(df['consonant_diacritic'])\n",
    "x_name = df['image_id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Test Split\n",
    "x_train_name, x_val_name, y_root_train, y_root_val, y_vowel_train, y_vowel_val, y_consonant_tarin, y_consonant_val = train_test_split(x_name, y_root, y_vowel, y_consonant, test_size=0.2, random_state=3) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((160672,),\n",
       " (40168,),\n",
       " (160672, 168),\n",
       " (40168, 168),\n",
       " (160672, 11),\n",
       " (40168, 11),\n",
       " (160672, 7),\n",
       " (40168, 7))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Verifying the shapes\n",
    "x_train_name.shape, x_val_name.shape, y_root_train.shape, y_root_val.shape, y_vowel_train.shape, y_vowel_val.shape, y_consonant_tarin.shape, y_consonant_val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the Data Generator\n",
    "def data_generator(x, y_root, y_vowel, y_consonant, batch_size=16, saved_img_path='./data/images/train', image_shape=(299, 299)):\n",
    "    assert len(x) == len(y_root) == len(y_vowel) == len(y_consonant), 'Lengths of all inputs should be same'\n",
    "    \n",
    "    num_splits = round(len(x) // batch_size) + 1\n",
    "    \n",
    "    x_splits = np.array_split(x, num_splits)\n",
    "    \n",
    "    print(\"Num Splits:\", len(x_splits))\n",
    "    \n",
    "    y_root_splits = np.array_split(y_root, num_splits)\n",
    "    y_vowel_splits = np.array_split(y_vowel, num_splits)\n",
    "    y_consonant_splits = np.array_split(y_consonant, num_splits)    \n",
    "    \n",
    "    i = 0\n",
    "    \n",
    "    while True:\n",
    "        xs = list()\n",
    "        \n",
    "        x_batch = x_splits[i].values\n",
    "        y_root_batch = y_root_splits[i].values\n",
    "        y_vowel_batch = y_vowel_splits[i].values\n",
    "        y_consonant_batch = y_consonant_splits[i].values\n",
    "        \n",
    "        print(\"Other Shapes: \", end=\"\")\n",
    "        print(x_batch.shape, y_root_batch.shape, y_vowel_batch.shape, y_consonant_batch.shape)\n",
    "        \n",
    "        i += 1\n",
    "        if i > num_splits-1: i = 0\n",
    "        \n",
    "        for x_ in x_batch:\n",
    "            path = os.path.join(saved_img_path, f\"{x_}.jpg\")\n",
    "            image = cv2.resize(cv2.imread(path, 0), image_shape, cv2.INTER_AREA)/255\n",
    "            xs.append(image)\n",
    "        \n",
    "        yield x_batch, np.array(xs), [y_root_batch, y_vowel_batch, y_consonant_batch]\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = Input(shape = (SIZE, SIZE))\n",
    "\n",
    "model = Conv2D(filters=32, kernel_size=(3, 3), padding='same', activation='relu', input_shape=(IMG_SIZE, IMG_SIZE, 1))(inputs)\n",
    "model = Conv2D(filters=32, kernel_size=(3, 3), padding='same', activation='relu')(model)\n",
    "model = BatchNormalization(momentum=0.15)(model)\n",
    "model = MaxPool2D(pool_size=(2, 2))(model)\n",
    "model = Conv2D(filters=32, kernel_size=(5, 5), padding='same', activation='relu')(model)\n",
    "model = Dropout(rate=0.3)(model)\n",
    "\n",
    "model = Conv2D(filters=64, kernel_size=(3, 3), padding='same', activation='relu')(model)\n",
    "model = Conv2D(filters=64, kernel_size=(3, 3), padding='same', activation='relu')(model)\n",
    "model = BatchNormalization(momentum=0.15)(model)\n",
    "model = MaxPool2D(pool_size=(2, 2))(model)\n",
    "model = Conv2D(filters=64, kernel_size=(5, 5), padding='same', activation='relu')(model)\n",
    "model = BatchNormalization(momentum=0.15)(model)\n",
    "model = Dropout(rate=0.3)(model)\n",
    "\n",
    "model = Conv2D(filters=128, kernel_size=(3, 3), padding='same', activation='relu')(model)\n",
    "model = Conv2D(filters=128, kernel_size=(3, 3), padding='same', activation='relu')(model)\n",
    "model = BatchNormalization(momentum=0.15)(model)\n",
    "model = MaxPool2D(pool_size=(2, 2))(model)\n",
    "model = Conv2D(filters=128, kernel_size=(5, 5), padding='same', activation='relu')(model)\n",
    "model = BatchNormalization(momentum=0.15)(model)\n",
    "model = Dropout(rate=0.3)(model)\n",
    "\n",
    "model = Flatten()(model)\n",
    "model = Dense(1024, activation = \"relu\")(model)\n",
    "model = Dropout(rate=0.3)(model)\n",
    "dense = Dense(512, activation = \"relu\")(model)\n",
    "\n",
    "head_root = Dense(168, activation = 'softmax')(dense)\n",
    "head_vowel = Dense(11, activation = 'softmax')(dense)\n",
    "head_consonant = Dense(7, activation = 'softmax')(dense)\n",
    "\n",
    "model = Model(inputs=inputs, outputs=[head_root, head_vowel, head_consonant])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:bengocr]",
   "language": "python",
   "name": "conda-env-bengocr-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
